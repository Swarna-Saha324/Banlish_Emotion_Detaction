{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13802091,"sourceType":"datasetVersion","datasetId":8787854}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:49:54.467803Z","iopub.execute_input":"2025-12-14T16:49:54.468516Z","iopub.status.idle":"2025-12-14T16:49:54.782335Z","shell.execute_reply.started":"2025-12-14T16:49:54.468483Z","shell.execute_reply":"2025-12-14T16:49:54.781540Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lastly/collect_preprocessed_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:34:46.470269Z","iopub.execute_input":"2025-12-14T17:34:46.470918Z","iopub.status.idle":"2025-12-14T17:34:46.475274Z","shell.execute_reply.started":"2025-12-14T17:34:46.470887Z","shell.execute_reply":"2025-12-14T17:34:46.474508Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/lastly/collect_preprocessed_dataset.csv\")\n\nemotion_cols = [\n    'Love', 'Joy', 'Anger', 'Surprise', 'Sadness', 'Fear', 'Hate'\n]\n\ndf = df[['Data'] + emotion_cols]\ndf.dropna(inplace=True)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:52:04.381182Z","iopub.execute_input":"2025-12-14T16:52:04.381794Z","iopub.status.idle":"2025-12-14T16:52:04.466469Z","shell.execute_reply.started":"2025-12-14T16:52:04.381768Z","shell.execute_reply":"2025-12-14T16:52:04.465887Z"}},"outputs":[{"name":"stdout","text":"                                                Data  Love  Joy  Anger  \\\n0                       shitkale pampers pore ghumai     0    0      0   \n1                           ekta dokane 40 lakh taka     0    0      0   \n2  ami ekta aghatojnit smriti somporke obogoto ho...     0    0      0   \n3            tuder opor hobe gojob na kar opore hobe     0    0      0   \n4  update deoyar por onek valo hoye gese godi 100...     1    0      0   \n\n   Surprise  Sadness  Fear  Hate  \n0         1        0     0     0  \n1         0        0     0     1  \n2         0        0     1     0  \n3         0        0     1     0  \n4         0        0     0     0  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:51:44.427003Z","iopub.execute_input":"2025-12-14T16:51:44.427302Z","iopub.status.idle":"2025-12-14T16:51:44.433420Z","shell.execute_reply.started":"2025-12-14T16:51:44.427279Z","shell.execute_reply":"2025-12-14T16:51:44.432585Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Data', 'Love', 'Joy', 'Anger', 'Surprise', 'Sadness', 'Fear', 'Hate',\n       'topic', 'Domain'],\n      dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:52:24.809185Z","iopub.execute_input":"2025-12-14T16:52:24.809900Z","iopub.status.idle":"2025-12-14T16:52:24.820119Z","shell.execute_reply.started":"2025-12-14T16:52:24.809875Z","shell.execute_reply":"2025-12-14T16:52:24.819342Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(train_df)\nval_ds   = Dataset.from_pandas(val_df)\ntest_ds  = Dataset.from_pandas(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:52:35.248874Z","iopub.execute_input":"2025-12-14T16:52:35.249172Z","iopub.status.idle":"2025-12-14T16:52:35.301164Z","shell.execute_reply.started":"2025-12-14T16:52:35.249150Z","shell.execute_reply":"2025-12-14T16:52:35.300598Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_name = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize(batch):\n    tokens = tokenizer(\n        batch[\"Data\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128\n    )\n\n    labels = []\n    for i in range(len(batch[\"Data\"])):\n        labels.append([float(batch[col][i]) for col in emotion_cols])\n\n    tokens[\"labels\"] = labels\n    return tokens\n\ntrain_ds = Dataset.from_pandas(train_df).map(tokenize, batched=True)\nval_ds   = Dataset.from_pandas(val_df).map(tokenize, batched=True)\ntest_ds  = Dataset.from_pandas(test_df).map(tokenize, batched=True)\n\ntrain_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nval_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:38:17.201871Z","iopub.execute_input":"2025-12-14T17:38:17.202211Z","iopub.status.idle":"2025-12-14T17:38:20.665901Z","shell.execute_reply.started":"2025-12-14T17:38:17.202188Z","shell.execute_reply":"2025-12-14T17:38:20.665267Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22246 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0dd1980dd246568a66e9b808a3bf5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2781 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00a7fead63e414a8b4feeb3c55b2cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2781 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19526e5d015a4aaba58ca7ed8eb2b599"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(emotion_cols),\n    problem_type=\"multi_label_classification\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:38:39.712748Z","iopub.execute_input":"2025-12-14T17:38:39.713472Z","iopub.status.idle":"2025-12-14T17:38:39.849486Z","shell.execute_reply.started":"2025-12-14T17:38:39.713447Z","shell.execute_reply":"2025-12-14T17:38:39.848931Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n    preds = (probs > 0.5).astype(int)\n\n    return {\n        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n        \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:38:44.727537Z","iopub.execute_input":"2025-12-14T17:38:44.728317Z","iopub.status.idle":"2025-12-14T17:38:44.733147Z","shell.execute_reply.started":"2025-12-14T17:38:44.728284Z","shell.execute_reply":"2025-12-14T17:38:44.732159Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import (\n    precision_score,\n    recall_score,\n    f1_score,\n    accuracy_score,\n    hamming_loss\n)\nimport torch\nimport numpy as np\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n\n    # logits → probabilities\n    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n\n    # binary predictions\n    preds = (probs > 0.5).astype(int)\n\n    # 1️⃣ Normal Accuracy (Subset / Exact Match)\n    subset_accuracy = accuracy_score(labels, preds)\n\n    # 2️⃣ Hamming Accuracy\n    hamming_accuracy = 1 - hamming_loss(labels, preds)\n\n    # 3️⃣ Macro F1\n    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n\n    return {\n        \"subset_accuracy\": subset_accuracy,   # Normal accuracy\n        \"hamming_accuracy\": hamming_accuracy, # Best for multi-label\n        \"macro_f1\": macro_f1,\n        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:29:45.397903Z","iopub.execute_input":"2025-12-14T18:29:45.398572Z","iopub.status.idle":"2025-12-14T18:29:45.404231Z","shell.execute_reply.started":"2025-12-14T18:29:45.398546Z","shell.execute_reply":"2025-12-14T18:29:45.403744Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./mbert_results\",\n    eval_strategy=\"epoch\",        # ⚠️ correct for your version\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=20,\n    weight_decay=0.01,\n    logging_steps=100,\n    report_to=\"none\"\n)\n\nprint(\"✅ training_args created\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:29:56.603053Z","iopub.execute_input":"2025-12-14T18:29:56.603344Z","iopub.status.idle":"2025-12-14T18:29:56.648452Z","shell.execute_reply.started":"2025-12-14T18:29:56.603324Z","shell.execute_reply":"2025-12-14T18:29:56.647887Z"}},"outputs":[{"name":"stdout","text":"✅ training_args created\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\nprint(\"✅ Trainer initialized\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:30:00.487741Z","iopub.execute_input":"2025-12-14T18:30:00.488411Z","iopub.status.idle":"2025-12-14T18:30:00.502512Z","shell.execute_reply.started":"2025-12-14T18:30:00.488389Z","shell.execute_reply":"2025-12-14T18:30:00.501757Z"}},"outputs":[{"name":"stdout","text":"✅ Trainer initialized\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_115/1720824852.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(train_ds[0][\"labels\"])\nprint(train_ds[0][\"labels\"].dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:30:04.412640Z","iopub.execute_input":"2025-12-14T18:30:04.413251Z","iopub.status.idle":"2025-12-14T18:30:04.419929Z","shell.execute_reply.started":"2025-12-14T18:30:04.413228Z","shell.execute_reply":"2025-12-14T18:30:04.419325Z"}},"outputs":[{"name":"stdout","text":"tensor([0., 0., 0., 0., 0., 1., 0.])\ntorch.float32\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:30:07.720502Z","iopub.execute_input":"2025-12-14T18:30:07.721169Z","iopub.status.idle":"2025-12-14T19:58:39.506841Z","shell.execute_reply.started":"2025-12-14T18:30:07.721141Z","shell.execute_reply":"2025-12-14T19:58:39.505753Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13911' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13911/27820 1:28:06 < 1:28:06, 2.63 it/s, Epoch 10/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Subset Accuracy</th>\n      <th>Hamming Accuracy</th>\n      <th>Macro F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.182800</td>\n      <td>0.285194</td>\n      <td>0.534700</td>\n      <td>0.891971</td>\n      <td>0.605072</td>\n      <td>0.676328</td>\n      <td>0.568342</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.149800</td>\n      <td>0.282676</td>\n      <td>0.571737</td>\n      <td>0.897365</td>\n      <td>0.648315</td>\n      <td>0.691980</td>\n      <td>0.618362</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.157500</td>\n      <td>0.276213</td>\n      <td>0.563466</td>\n      <td>0.897005</td>\n      <td>0.650881</td>\n      <td>0.692650</td>\n      <td>0.617620</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.128600</td>\n      <td>0.288300</td>\n      <td>0.588997</td>\n      <td>0.900601</td>\n      <td>0.667322</td>\n      <td>0.705356</td>\n      <td>0.641442</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.103700</td>\n      <td>0.320816</td>\n      <td>0.599065</td>\n      <td>0.899625</td>\n      <td>0.663487</td>\n      <td>0.690533</td>\n      <td>0.644883</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.087100</td>\n      <td>0.325917</td>\n      <td>0.612729</td>\n      <td>0.903837</td>\n      <td>0.679847</td>\n      <td>0.709263</td>\n      <td>0.654964</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.069300</td>\n      <td>0.354146</td>\n      <td>0.609853</td>\n      <td>0.900755</td>\n      <td>0.680078</td>\n      <td>0.685487</td>\n      <td>0.678329</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.049800</td>\n      <td>0.378005</td>\n      <td>0.612729</td>\n      <td>0.902193</td>\n      <td>0.680690</td>\n      <td>0.696233</td>\n      <td>0.669074</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.046100</td>\n      <td>0.390712</td>\n      <td>0.635023</td>\n      <td>0.905738</td>\n      <td>0.694355</td>\n      <td>0.711952</td>\n      <td>0.681400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.035300</td>\n      <td>0.423983</td>\n      <td>0.624955</td>\n      <td>0.903632</td>\n      <td>0.688993</td>\n      <td>0.695933</td>\n      <td>0.684394</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/2: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_115/49973641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 683460288 vs 683460176"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 683460288 vs 683460176","output_type":"error"}],"execution_count":36}]}
