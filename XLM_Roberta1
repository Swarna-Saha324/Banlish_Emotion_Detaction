{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13802091,"sourceType":"datasetVersion","datasetId":8787854}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:58:17.476935Z","iopub.execute_input":"2025-11-24T18:58:17.477176Z","iopub.status.idle":"2025-11-24T18:58:17.482846Z","shell.execute_reply.started":"2025-11-24T18:58:17.477153Z","shell.execute_reply":"2025-11-24T18:58:17.482245Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lastly/collect_preprocessed_dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    hamming_loss, jaccard_score\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:58:46.195062Z","iopub.execute_input":"2025-11-24T18:58:46.195695Z","iopub.status.idle":"2025-11-24T18:58:46.200289Z","shell.execute_reply.started":"2025-11-24T18:58:46.195669Z","shell.execute_reply":"2025-11-24T18:58:46.199507Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lastly/collect_preprocessed_dataset.csv\")\n\nlabel_cols = ['Love','Joy','Anger','Surprise','Sadness','Fear','Hate']\ndf[label_cols] = df[label_cols].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:59:19.781604Z","iopub.execute_input":"2025-11-24T18:59:19.782341Z","iopub.status.idle":"2025-11-24T18:59:19.898118Z","shell.execute_reply.started":"2025-11-24T18:59:19.782316Z","shell.execute_reply":"2025-11-24T18:59:19.897546Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_texts = train_df[\"Data\"].tolist()\nval_texts   = val_df[\"Data\"].tolist()\n\ntrain_labels = train_df[label_cols].values\nval_labels   = val_df[label_cols].values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:59:31.099004Z","iopub.execute_input":"2025-11-24T18:59:31.099298Z","iopub.status.idle":"2025-11-24T18:59:31.116023Z","shell.execute_reply.started":"2025-11-24T18:59:31.099276Z","shell.execute_reply":"2025-11-24T18:59:31.115466Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"MODEL = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ndef encode_batch(texts):\n    return tokenizer(\n        texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:59:41.827097Z","iopub.execute_input":"2025-11-24T18:59:41.827605Z","iopub.status.idle":"2025-11-24T18:59:44.626265Z","shell.execute_reply.started":"2025-11-24T18:59:41.827581Z","shell.execute_reply":"2025-11-24T18:59:44.625593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf317235a5c40a6b3d6ebaad428ce74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ffe13564c5744e2ab6d8f50eacfc3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b592f692a00b466b9184901f1462b1a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a7457579ee45c0bbd783d0c2e2e537"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class EmotionDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels):\n        self.enc = encode_batch(texts)\n        self.labels = torch.tensor(labels, dtype=torch.float)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k,v in self.enc.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\ntrain_ds = EmotionDataset(train_texts, train_labels)\nval_ds   = EmotionDataset(val_texts, val_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:00:04.637882Z","iopub.execute_input":"2025-11-24T19:00:04.638178Z","iopub.status.idle":"2025-11-24T19:00:04.686056Z","shell.execute_reply.started":"2025-11-24T19:00:04.638158Z","shell.execute_reply":"2025-11-24T19:00:04.685057Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_134/109950031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmotionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mval_ds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mEmotionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_134/109950031.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, texts, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEmotionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_134/291186738.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     return tokenizer(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m                 )\n\u001b[1;32m   2942\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2943\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2944\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3142\u001b[0m         )\n\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3144\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3145\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"],"ename":"TypeError","evalue":"TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"class EmotionDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels):\n        # texts must be a LIST of python strings\n        texts = list(texts)\n\n        self.enc = encode_batch(texts)\n        self.labels = torch.tensor(labels, dtype=torch.float)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:00:37.181373Z","iopub.execute_input":"2025-11-24T19:00:37.182238Z","iopub.status.idle":"2025-11-24T19:00:37.187268Z","shell.execute_reply.started":"2025-11-24T19:00:37.182212Z","shell.execute_reply":"2025-11-24T19:00:37.186477Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL,\n    num_labels=len(label_cols),\n    problem_type=\"multi_label_classification\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:01:00.570595Z","iopub.execute_input":"2025-11-24T19:01:00.571104Z","iopub.status.idle":"2025-11-24T19:01:04.879713Z","shell.execute_reply.started":"2025-11-24T19:01:00.571082Z","shell.execute_reply":"2025-11-24T19:01:04.879044Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d288dc35994d14b58e7b6382f685da"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = 1 / (1 + np.exp(-logits))     # sigmoid\n    preds = (probs > 0.5).astype(int)\n\n    return {\n        \"exact_acc\": accuracy_score(labels, preds),\n        \"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n        \"recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n        \"hamming_loss\": hamming_loss(labels, preds),\n        \"jaccard_samples\": jaccard_score(labels, preds, average=\"samples\")\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:01:16.903967Z","iopub.execute_input":"2025-11-24T19:01:16.904747Z","iopub.status.idle":"2025-11-24T19:01:16.909464Z","shell.execute_reply.started":"2025-11-24T19:01:16.904720Z","shell.execute_reply":"2025-11-24T19:01:16.908811Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"TEXT_COL = \"Data\"\nLABEL_COLS = ['Love','Joy','Anger','Surprise','Sadness','Fear','Hate']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:04:56.718698Z","iopub.execute_input":"2025-11-24T19:04:56.719488Z","iopub.status.idle":"2025-11-24T19:04:56.723245Z","shell.execute_reply.started":"2025-11-24T19:04:56.719458Z","shell.execute_reply":"2025-11-24T19:04:56.722401Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\nMODEL_NAME = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass EmotionDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len\n        )\n\n        item = {key: torch.tensor(val) for key, val in enc.items()}\n        item[\"labels\"] = self.labels[idx]\n\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:05:06.390630Z","iopub.execute_input":"2025-11-24T19:05:06.391362Z","iopub.status.idle":"2025-11-24T19:05:08.358223Z","shell.execute_reply.started":"2025-11-24T19:05:06.391338Z","shell.execute_reply":"2025-11-24T19:05:08.357294Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_ds = EmotionDataset(train_texts, train_labels, tokenizer)\nval_ds   = EmotionDataset(val_texts,   val_labels,   tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:05:17.948453Z","iopub.execute_input":"2025-11-24T19:05:17.949173Z","iopub.status.idle":"2025-11-24T19:05:17.953418Z","shell.execute_reply.started":"2025-11-24T19:05:17.949150Z","shell.execute_reply":"2025-11-24T19:05:17.952723Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/xlmr-baseline\",\n   eval_strategy=\"epoch\",      # OLD name\n    save_strategy=\"epoch\",  \n    save_total_limit=1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=20,\n    weight_decay=0.01,\n    logging_steps=100,\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:06:04.042167Z","iopub.execute_input":"2025-11-24T19:06:04.042782Z","iopub.status.idle":"2025-11-24T19:06:04.075733Z","shell.execute_reply.started":"2025-11-24T19:06:04.042761Z","shell.execute_reply":"2025-11-24T19:06:04.074961Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:06:18.269594Z","iopub.execute_input":"2025-11-24T19:06:18.270203Z","iopub.status.idle":"2025-11-24T19:06:18.818722Z","shell.execute_reply.started":"2025-11-24T19:06:18.270182Z","shell.execute_reply":"2025-11-24T19:06:18.817903Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_134/3278098785.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:06:45.292247Z","iopub.execute_input":"2025-11-24T19:06:45.292555Z","iopub.status.idle":"2025-11-24T23:12:55.501735Z","shell.execute_reply.started":"2025-11-24T19:06:45.292533Z","shell.execute_reply":"2025-11-24T23:12:55.500896Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27820' max='27820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [27820/27820 4:06:07, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Acc</th>\n      <th>Precision Macro</th>\n      <th>Recall Macro</th>\n      <th>F1 Macro</th>\n      <th>Hamming Loss</th>\n      <th>Jaccard Samples</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.331800</td>\n      <td>0.316319</td>\n      <td>0.286048</td>\n      <td>0.653976</td>\n      <td>0.296857</td>\n      <td>0.349185</td>\n      <td>0.130631</td>\n      <td>0.302020</td>\n      <td>52.377500</td>\n      <td>106.191000</td>\n      <td>6.644000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.293800</td>\n      <td>0.279821</td>\n      <td>0.413700</td>\n      <td>0.680249</td>\n      <td>0.463551</td>\n      <td>0.537784</td>\n      <td>0.117840</td>\n      <td>0.444025</td>\n      <td>52.450900</td>\n      <td>106.042000</td>\n      <td>6.635000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.256000</td>\n      <td>0.266539</td>\n      <td>0.458288</td>\n      <td>0.707038</td>\n      <td>0.501516</td>\n      <td>0.579157</td>\n      <td>0.111368</td>\n      <td>0.486800</td>\n      <td>52.283900</td>\n      <td>106.381000</td>\n      <td>6.656000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.233000</td>\n      <td>0.257620</td>\n      <td>0.521036</td>\n      <td>0.701260</td>\n      <td>0.568490</td>\n      <td>0.616546</td>\n      <td>0.105024</td>\n      <td>0.553248</td>\n      <td>52.297100</td>\n      <td>106.354000</td>\n      <td>6.654000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.203500</td>\n      <td>0.247522</td>\n      <td>0.548004</td>\n      <td>0.709178</td>\n      <td>0.591619</td>\n      <td>0.643349</td>\n      <td>0.100837</td>\n      <td>0.580846</td>\n      <td>52.484100</td>\n      <td>105.975000</td>\n      <td>6.631000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.186200</td>\n      <td>0.252519</td>\n      <td>0.573535</td>\n      <td>0.699687</td>\n      <td>0.624366</td>\n      <td>0.656806</td>\n      <td>0.099707</td>\n      <td>0.609193</td>\n      <td>52.336100</td>\n      <td>106.275000</td>\n      <td>6.649000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.145500</td>\n      <td>0.268129</td>\n      <td>0.587379</td>\n      <td>0.695000</td>\n      <td>0.650671</td>\n      <td>0.668301</td>\n      <td>0.099502</td>\n      <td>0.630109</td>\n      <td>53.094600</td>\n      <td>104.756000</td>\n      <td>6.554000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.136900</td>\n      <td>0.286784</td>\n      <td>0.587558</td>\n      <td>0.687321</td>\n      <td>0.642992</td>\n      <td>0.661528</td>\n      <td>0.100580</td>\n      <td>0.624431</td>\n      <td>53.138500</td>\n      <td>104.670000</td>\n      <td>6.549000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.113000</td>\n      <td>0.283567</td>\n      <td>0.596548</td>\n      <td>0.696723</td>\n      <td>0.657779</td>\n      <td>0.674593</td>\n      <td>0.098089</td>\n      <td>0.637346</td>\n      <td>53.121500</td>\n      <td>104.703000</td>\n      <td>6.551000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.097500</td>\n      <td>0.289976</td>\n      <td>0.613448</td>\n      <td>0.699086</td>\n      <td>0.678321</td>\n      <td>0.687635</td>\n      <td>0.095264</td>\n      <td>0.654006</td>\n      <td>52.849700</td>\n      <td>105.242000</td>\n      <td>6.585000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.088600</td>\n      <td>0.308946</td>\n      <td>0.605897</td>\n      <td>0.690173</td>\n      <td>0.669494</td>\n      <td>0.676628</td>\n      <td>0.098192</td>\n      <td>0.646305</td>\n      <td>53.133100</td>\n      <td>104.681000</td>\n      <td>6.550000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.083700</td>\n      <td>0.332391</td>\n      <td>0.610572</td>\n      <td>0.685107</td>\n      <td>0.685779</td>\n      <td>0.684012</td>\n      <td>0.096933</td>\n      <td>0.656619</td>\n      <td>52.859000</td>\n      <td>105.223000</td>\n      <td>6.584000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.067700</td>\n      <td>0.328430</td>\n      <td>0.621179</td>\n      <td>0.702160</td>\n      <td>0.687724</td>\n      <td>0.692792</td>\n      <td>0.093928</td>\n      <td>0.662861</td>\n      <td>53.119900</td>\n      <td>104.706000</td>\n      <td>6.551000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.062900</td>\n      <td>0.351729</td>\n      <td>0.617584</td>\n      <td>0.693222</td>\n      <td>0.686605</td>\n      <td>0.685640</td>\n      <td>0.095315</td>\n      <td>0.659340</td>\n      <td>53.179300</td>\n      <td>104.590000</td>\n      <td>6.544000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.055800</td>\n      <td>0.358927</td>\n      <td>0.628910</td>\n      <td>0.701271</td>\n      <td>0.692599</td>\n      <td>0.694890</td>\n      <td>0.092849</td>\n      <td>0.666987</td>\n      <td>53.131300</td>\n      <td>104.684000</td>\n      <td>6.550000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.046500</td>\n      <td>0.370121</td>\n      <td>0.621539</td>\n      <td>0.689346</td>\n      <td>0.697186</td>\n      <td>0.690925</td>\n      <td>0.094956</td>\n      <td>0.665363</td>\n      <td>52.863500</td>\n      <td>105.214000</td>\n      <td>6.583000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.045000</td>\n      <td>0.377986</td>\n      <td>0.630708</td>\n      <td>0.701775</td>\n      <td>0.697247</td>\n      <td>0.699139</td>\n      <td>0.092361</td>\n      <td>0.669858</td>\n      <td>52.950800</td>\n      <td>105.041000</td>\n      <td>6.572000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.033000</td>\n      <td>0.382448</td>\n      <td>0.625315</td>\n      <td>0.696551</td>\n      <td>0.696283</td>\n      <td>0.696179</td>\n      <td>0.093980</td>\n      <td>0.665004</td>\n      <td>52.232300</td>\n      <td>106.486000</td>\n      <td>6.663000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.037500</td>\n      <td>0.391138</td>\n      <td>0.624955</td>\n      <td>0.695195</td>\n      <td>0.699584</td>\n      <td>0.696920</td>\n      <td>0.093851</td>\n      <td>0.666727</td>\n      <td>52.464200</td>\n      <td>106.015000</td>\n      <td>6.633000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.033300</td>\n      <td>0.393578</td>\n      <td>0.626214</td>\n      <td>0.697898</td>\n      <td>0.698172</td>\n      <td>0.697403</td>\n      <td>0.093466</td>\n      <td>0.666921</td>\n      <td>52.472100</td>\n      <td>105.999000</td>\n      <td>6.632000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=27820, training_loss=0.13071626883715198, metrics={'train_runtime': 14769.8381, 'train_samples_per_second': 30.125, 'train_steps_per_second': 1.884, 'total_flos': 2.9268472132992e+16, 'train_loss': 0.13071626883715198, 'epoch': 20.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/xlmr_emotion\")\ntokenizer.save_pretrained(\"/kaggle/working/xlmr_emotion\")\nprint(\"Model saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T23:28:40.656762Z","iopub.execute_input":"2025-11-24T23:28:40.657071Z","iopub.status.idle":"2025-11-24T23:28:43.426814Z","shell.execute_reply.started":"2025-11-24T23:28:40.657050Z","shell.execute_reply":"2025-11-24T23:28:43.426016Z"}},"outputs":[{"name":"stdout","text":"Model saved!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def predict(text):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"pt\"\n    )\n\n    with torch.no_grad():\n        logits = model(**inputs).logits.cpu().numpy()[0]\n\n    probs = 1 / (1 + np.exp(-logits))\n    binary = (probs > 0.5).astype(int)\n\n    return {\n        \"probabilities\": dict(zip(label_cols, probs)),\n        \"final_labels\": dict(zip(label_cols, binary))\n    }\n\nprint(predict(\"I am very happy today\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}